{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scholarly\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "os.chdir('C:\\\\Users\\\\Cafral\\\\Desktop\\\\kaggle\\\\CORD-19-research-challenge\\\\data_v7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('clean_metadata.csv')\n",
    "metadata.rename(columns={'sha':'paper_id'}, inplace = True)\n",
    "metadata['paper_id'] = metadata['paper_id'].astype(\"str\")\n",
    "metadata['title'] = metadata['title'].fillna('nan')\n",
    "metadata['abstract'] = metadata['abstract'].fillna('nan')\n",
    "\n",
    "metadata['text'] = metadata['title']+ ' ' + metadata['abstract']\n",
    "metadata.drop_duplicates(['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting papers which contain ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(dataframe,columnToSearch,keywords):\n",
    "    df_w_ngrams = dataframe[dataframe[columnToSearch].str.contains('|'.join(keywords), case=False) == True]\n",
    "    return df_w_ngrams\n",
    "\n",
    "ngrams = ['population density','number of people in','number of people per',\n",
    "                             'highly populated areas','highly populated countries',\n",
    "                             'densely populated countries','densely populated areas',\n",
    "                             'high density areas','high density countries'\n",
    "                             ,'population densities', 'density of population','sparsely populated',\n",
    "                             'densely populated','density of the population','dense population','populated areas',\n",
    "                             'densely inhabited','housing density','densely-populated','concentration of people',\n",
    "                             'population pressure','population studies','populated regions','populous',\n",
    "                             'high population densities','residential densities','overpopulated']\n",
    "\n",
    "metadata_ngrams = find_ngrams(metadata,'text',ngrams)\n",
    "#ngrams_titles.rename(columns={'title':'title_w_ngram'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting all sentences from the relevant papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_sentences(dataFrame):\n",
    "    paper_ids = list(dataFrame['paper_id'].unique())\n",
    "    meta_sent_df = pd.DataFrame(columns=['paper_id','sentence'])\n",
    "    for paper_id in paper_ids:\n",
    "        sentence_df = pd.DataFrame(columns=['paper_id','sentence'])\n",
    "        paper_id_df = dataFrame[dataFrame['paper_id']==paper_id]\n",
    "\n",
    "        for idx_num,row in paper_id_df.iterrows():\n",
    "            sentences = [sentence for sentence in sent_tokenize(row.abstract)]\n",
    "        sentence_df['sentence'] = sentences\n",
    "        #print(sentence_df['sentence'])\n",
    "        sentence_df['paper_id'] = paper_id\n",
    "        meta_sent_df = pd.concat([meta_sent_df,sentence_df])\n",
    "\n",
    "    meta_sent_df.reset_index(inplace=True)\n",
    "    meta_sent_df.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    return meta_sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_sent_df = metadata_sentences(metadata_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting methodolody,sample size,causal nature,sentences refering to coronavirus, fatality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(ngramDf,allSentdataFrame,ngrams):\n",
    "    \n",
    "    #ngram sentences\n",
    "    sentences = find_ngrams(allSentdataFrame,'sentence',ngrams)\n",
    "    \n",
    "    # extracting methodology\n",
    "    methods_list = ['regression','OLS','logistic','time series','model','modelling','simulation','forecast','forecasting']\n",
    "    methodology = find_ngrams(allSentdataFrame,'sentence',methods_list)\n",
    "\n",
    "    #extracting sample size\n",
    "    sample_size_list = ['population size','sample size','number of samples','number of observations','number of subjects']\n",
    "    sample_size = find_ngrams(allSentdataFrame,'sentence',sample_size_list)\n",
    "\n",
    "    #extracting nature of correlation\n",
    "    causal_list =['statistically significant','statistical significance',\n",
    "                  'correlation','positively correlated','negatively correlated','correlated',\n",
    "                  'p value','p-value','chi square','chi-square',\n",
    "                  'confidence interval','CI','odds ratio','OR','coefficient']\n",
    "\n",
    "    causality_type = find_ngrams(allSentdataFrame,'sentence',causal_list)\n",
    "\n",
    "    # extracting coronavirus related sentence #can someone check and update this list?\n",
    "    coronavirus_list = ['severe acute respiratory syndrome','sars-cov','sars-like',\n",
    "                        'middle east respiratory syndrome','mers-cov','mers-like',\n",
    "                        'covid-19','sars-cov-2','2019-ncov','sars-2',\n",
    "                        'sarscov-2','novel coronavirus','corona virus','coronaviruses',\n",
    "                        'sars','mers','covid19','covid 19']\n",
    "\n",
    "    coronavirus = find_ngrams(allSentdataFrame,'sentence',coronavirus_list)\n",
    "\n",
    "    # extracting outcome\n",
    "    disease_stage_list = ['lethal', 'morbid',\"death\", \"fatality\", \"mortality\",\"lethal\", \"lethality\", \"morbidity\"]\n",
    "\n",
    "    fatality = find_ngrams(allSentdataFrame,'sentence',disease_stage_list)\n",
    "\n",
    "    df_list = [sentences,methodology,sample_size,causality_type,coronavirus,fatality]\n",
    "    df_list_name = ['sentences','methodology','sample_size','causality_type','coronavirus','fatality']\n",
    "    i=0\n",
    "    for one_df in df_list:\n",
    "        one_df.rename(columns={'sentence':df_list_name[i]},inplace=True)\n",
    "        grouped_one_df = one_df.groupby(['paper_id'], as_index=False)[df_list_name[i]].sum()\n",
    "        ngramDf = pd.merge(ngramDf,grouped_one_df,on='paper_id',how='left')\n",
    "        i=i+1\n",
    "    return ngramDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cafral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "metadata_ngrams = extract_features(metadata_ngrams,meta_sent_df,ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_ngrams.drop(columns=['Unnamed: 0', 'cord_uid','source_x','pmcid', 'pubmed_id', 'license', \n",
    "                              'Microsoft Academic Paper ID', 'WHO #Covidence',\n",
    "                              'has_full_text', 'full_text_file'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cafral\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def keywordcounter(sentences, keywords_list):\n",
    "    '''\n",
    "    Input : List of sentences, List of keywords\n",
    "    Returns : Keywords present in sentences, Total count of all keywords present in Input\n",
    "    '''\n",
    "    keyword = {}\n",
    "    sent = \" \".join(sentences)\n",
    "    for pol in keywords_list:\n",
    "        counter = sent.lower().count(pol)\n",
    "        if (counter > 0):\n",
    "            keyword[pol] = counter\n",
    "    return list(keyword.keys()), sum(keyword.values())\n",
    "\n",
    "def aggregation(item,keyWordList,RiskFactor):\n",
    "    '''\n",
    "    Input : Dataframe of sentences of a paper\n",
    "    Return : Datframe in Standard Output format\n",
    "    '''\n",
    "    dfo = {}\n",
    "    \n",
    "    dfo['Risk Factor'] = RiskFactor\n",
    "    dfo['Title'] = item['title'].iloc[0]\n",
    "    dfo['Keyword/Ngram'], dfo['No of keyword occurence in Paper'] = keywordcounter(item['text'].tolist(),\n",
    "                                                                                 keyWordList)\n",
    "    dfo['Sentences'] = item['sentences'].iloc[0]\n",
    "    dfo['paper_id'] = item['paper_id'].iloc[0]\n",
    "    \n",
    "    dfo['URL'] = item['url'].iloc[0]\n",
    "    \n",
    "    dfo['Authors'] = item['authors'].iloc[0]\n",
    "\n",
    "    try:\n",
    "        dfo['No of Citations'] = next(scholarly.search_pubs_query(item['title'].iloc[0])).citedby\n",
    "    except:\n",
    "        dfo['No of Citations'] = 0\n",
    "        \n",
    "    dfo['Correlation'] = item['causality_type'].iloc[0]\n",
    "    dfo['Design Methodology'] = item['methodology'].iloc[0]\n",
    "    dfo['Sample Size'] = item['sample_size'].iloc[0]\n",
    "    dfo['Coronavirus'] = item['coronavirus'].iloc[0]\n",
    "    dfo['Fatality'] = item['fatality'].iloc[0]\n",
    "    #dfo['TAXON'] =item['TAXON'].iloc[0]\n",
    "    \n",
    "    return dfo\n",
    "\n",
    "#del df_output\n",
    "df_output = pd.DataFrame(columns=['Risk Factor', 'Title','Keyword/Ngram', 'No of keyword occurence in Paper',\n",
    "                                  'paper_id', 'URL',\n",
    "                                  'Authors','No of Citations', 'Correlation', \n",
    "                                  'Design Methodology', 'Sample Size',\n",
    "                                 'Coronavirus','Fatality'])\n",
    "\n",
    "grouped = metadata_ngrams.groupby('paper_id')\n",
    "for key, item in grouped:\n",
    "    df_output = pd.concat([df_output, pd.DataFrame([aggregation(item,ngrams,'Population Density')])])\n",
    "\n",
    "df_output = df_output.reset_index()\n",
    "df_output.to_excel('population_density_metadata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
